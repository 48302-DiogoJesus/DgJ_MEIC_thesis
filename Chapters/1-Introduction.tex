% #############################################################################
% This is Chapter 1
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\chapter{Introduction}
\chaptoc
% The following line allows to ref this chapter
\label{chap:intro}


Function-as-a-Service (FaaS) represents a serverless cloud computing paradigm that simplifies application deployment by abstracting away infrastructure management. It provides automatic, elastic scalability—potentially without limit—along with a fine-grained, pay-per-use pricing model. This has led to its widespread adoption for event-driven systems, microservices, and web services on platforms like AWS Lambda\footnote{\label{fn:aws-lambda}https://aws.amazon.com/pt/lambda/}, Azure Functions\footnote{\label{fn:azure-funcs}https://azure.microsoft.com/en-us/products/functions}, and Google Cloud Functions\footnote{\label{fn:google-cloud-functions}https://cloud.google.com/functions}. These applications typically benefit the most from FaaS because they are lightweight, stateless, and characterized by highly variable or unpredictable workloads, allowing them to leverage serverless platforms' on-demand scalability and cost-efficiency.

This paradigm is also increasingly used to execute complex scientific and data processing workflows, such as the Cybershake~\cite{cybershake_workflow} seismic hazard analysis or Montage~\cite{montage_astronomy}, an astronomy image mosaicking workflow. These applications are structured as workflows—formally represented as Directed Acyclic Graphs (DAGs) of interdependent tasks. However, efficiently executing these complex workflows on serverless platforms remains a significant challenge. 

Despite their advantages, serverless platforms present several limitations that complicate the execution of complex workflows. Since these platforms allow scaling down to zero resources to save costs, they can also introduce unpredictable latency, known as \textit{cold starts}~\cite{cold_starts_surey}, particularly for short-lived functions, affecting overall workflow performance. The lack of \textit{direct inter-function communication}~\cite{serverless_computing_drawbacks_survey_rw1} means that tasks often have to rely on external services, such as message brokers or databases to exchange intermediate data, which can increase overhead and reduce efficiency. Interoperability between platforms is further limited by the use of platform-specific workflow definition languages, which restricts the portability of workflows across different serverless environments. Additionally, while statelessness simplifies scaling and management, it can introduce overhead and complexity for applications that require continuity or coordination across multiple function invocations. Finally, developers have limited control over the underlying infrastructure, restricting the ability to optimize resource usage or tune performance for specific workloads.

Several solutions have emerged to address the limitations of serverless platforms. Stateful functions (e.g., AWS Step Functions\footnote{\label{fn:aws-step-functions}https://aws.amazon.com/pt/step-functions/}, Azure Durable Functions\footnote{\label{fn:azure-durable-functions}https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview?tabs=in-process\%2Cnodejs-v3\%2Cv1-model\&pivots=csharp}, and Google Cloud Workflows\footnote{\label{fn:google-cloud-workflows}https://cloud.google.com/workflows}) expand the range of applications that can run on serverless platforms by maintaining state across multiple function invocations, coordinating complex workflows, and providing built-in fault tolerance. Other approaches tackle limitations at the runtime level, proposing extensions to FaaS platforms (e.g., Faa\$T~\cite{faast_caching}, Palette~\cite{palette_load_balancing}, Lambdata~\cite{lambdata_intents}) or entirely new serverless architectures (e.g., Apache OpenWhisk~\cite{open_whisk}). Finally, some workflow-focused solutions (e.g., WUKONG~\cite{wukong_2}, Unum~\cite{unum_decentralized_orchestrator}, DEWEv3~\cite{dewe_v3}) employ scheduling strategies and workflow-level optimizations to enhance efficiency, primarily by improving data locality to bring computation closer to the data and minimize reliance on external services.

These workflow-focused approaches, however, often use uniform resources for workers and rely on “one-step scheduling,” making decisions based solely on the immediate workflow stage without considering the broader context or the downstream effects of their decisions. This combination of homogeneous worker configurations and limited scheduling foresight can lead to inefficient use of resources when tasks have diverse computational or memory requirements. Furthermore, the heuristic-based approaches used by other solutions can be inefficient in certain scenarios, as they lack mechanisms to adapt worker resource allocations to the specific needs of individual tasks. Moreover, we found no prior work that leverages metadata or historical metrics to inform scheduling decisions across an entire serverless workflow.  

This limitation motivates the central research question of this work: if we have knowledge of the computation steps, collect sufficient metrics on their behavior, and understand how they are composed to form the full workflow, can we leverage this information to make smarter scheduling decisions that minimize makespan and maximize resource efficiency in a FaaS environment?  

To answer this research question, we propose a decentralized serverless workflow execution engine that leverages historical metadata from previous workflow runs to generate informed task allocation plans, which are then executed by FaaS workers in a choreographed manner, without needing a central scheduler. By relying on such planning, our approach aims to minimize the usage of external cloud storage services, which are often employed by similar solutions for intermediate data exchange and synchronization, while also avoiding the inefficiencies of homogeneous worker resource allocations.

The main contributions of this work are as follows:
\begin{itemize}
    \item Analysis of the serverless workflow orchestration research landscape;
    \item Propose a decentralized serverless workflow execution engine that overcomes the "one-step scheduling" and uniform-resource limitations of existing workflow-focused solutions by leveraging historical metadata to generate informed execution plans;
    \item Demonstrate how incorporating historical execution data can improve task allocation, reduce reliance on external cloud storage services, and enhance overall workflow efficiency on FaaS platforms.
\end{itemize}

% #############################################################################
\section{Problem/Motivation}
% see intro of paper and pic (if needed)

% #############################################################################
\section{Gaps in prior work}
% see intro of paper

% #############################################################################
\section{Proposed Solution}
% see intro of paper

% #############################################################################
\section{Document Organization}
